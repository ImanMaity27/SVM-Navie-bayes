{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression** tasks.\n",
        "\n",
        "* It is primarily used for **binary classification**, but can be extended to multiclass problems.\n",
        "* SVM tries to find the **optimal boundary (hyperplane)** that separates classes in the feature space.\n",
        "\n",
        "---\n",
        "\n",
        "### **How SVM Works**\n",
        "\n",
        "1. **Separating Hyperplane:**\n",
        "\n",
        "   * For linearly separable data, SVM finds a **hyperplane** that separates the two classes.\n",
        "   * In 2D, this is a line; in 3D, it’s a plane; in higher dimensions, it’s a hyperplane.\n",
        "\n",
        "2. **Maximum Margin:**\n",
        "\n",
        "   * Among all possible hyperplanes, SVM selects the one that **maximizes the margin** — the distance between the hyperplane and the **closest data points** from each class (called **support vectors**).\n",
        "   * Maximizing the margin helps **generalization** on unseen data.\n",
        "\n",
        "3. **Support Vectors:**\n",
        "\n",
        "   * Only the points **closest to the hyperplane** influence its position.\n",
        "   * These are critical in defining the decision boundary.\n",
        "\n",
        "4. **Non-linear Data:**\n",
        "\n",
        "   * If data is not linearly separable, SVM uses a **kernel trick** to map data into a **higher-dimensional space** where it becomes linearly separable.\n",
        "   * Common kernels:\n",
        "\n",
        "     * **Linear**\n",
        "     * **Polynomial**\n",
        "     * **Radial Basis Function (RBF / Gaussian)**\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Advantages**\n",
        "\n",
        "* Effective in high-dimensional spaces.\n",
        "* Works well even when the number of features > number of samples.\n",
        "* Robust against overfitting (especially with proper regularization).\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuitive Example**\n",
        "\n",
        "* Imagine plotting two types of flowers on a 2D plane.\n",
        "* SVM finds the **line** that separates the two flower types **with the largest gap**.\n",
        "* Only the flowers closest to the line (support vectors) determine where the line is drawn.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Hard Margin vs Soft Margin SVM**\n",
        "\n",
        "SVM aims to find a hyperplane that separates classes, but the **approach depends on whether the data is perfectly separable or not**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Hard Margin SVM**\n",
        "\n",
        "* **Definition:**\n",
        "\n",
        "  * Assumes that the data is **perfectly linearly separable**.\n",
        "  * The SVM finds a hyperplane that **strictly separates all points** without any misclassification.\n",
        "* **Characteristics:**\n",
        "\n",
        "  * No points are allowed inside the margin.\n",
        "  * Very sensitive to **outliers** — a single misclassified point can make it impossible to find a hyperplane.\n",
        "* **Use Case:**\n",
        "\n",
        "  * Rare in real-world data because data is usually noisy.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Soft Margin SVM**\n",
        "\n",
        "* **Definition:**\n",
        "\n",
        "  * Allows some points to **violate the margin** or even be misclassified.\n",
        "  * Introduces a **slack variable** ($\\xi_i$) to measure the degree of violation.\n",
        "* **Characteristics:**\n",
        "\n",
        "  * Controlled by a **regularization parameter $C$**:\n",
        "\n",
        "    * **Large C:** Less tolerance for misclassification → smaller margin.\n",
        "    * **Small C:** More tolerance → wider margin, better generalization.\n",
        "  * More robust to **noise and outliers**.\n",
        "* **Use Case:**\n",
        "\n",
        "  * Most practical SVM applications, since real-world data is rarely perfectly separable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Difference Table**\n",
        "\n",
        "| Feature                 | Hard Margin              | Soft Margin                      |\n",
        "| ----------------------- | ------------------------ | -------------------------------- |\n",
        "| Separability            | Perfectly separable      | Allows some misclassification    |\n",
        "| Margin Violations       | Not allowed              | Allowed via slack variables      |\n",
        "| Sensitivity to Outliers | Very high                | Lower, more robust               |\n",
        "| Parameter               | No regularization needed | Regularization controlled by $C$ |\n",
        "\n",
        "---\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "* **Hard margin:** Draw a line that separates every point exactly.\n",
        "* **Soft margin:** Draw a line that separates most points but allows a few “exceptions” to improve generalization.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **What is the Kernel Trick in SVM?**\n",
        "\n",
        "* In SVM, **linear separation** works only when classes are linearly separable.\n",
        "* **Kernel Trick** allows SVM to handle **non-linear data** by implicitly mapping it into a **higher-dimensional space** without computing the coordinates explicitly.\n",
        "* This makes it possible to find a **linear hyperplane in the transformed space**, which corresponds to a **non-linear boundary in the original space**.\n",
        "\n",
        "**Mathematically:**\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $K$ is the kernel function\n",
        "* $\\phi(x)$ maps the original features into higher dimensions\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Kernel: Radial Basis Function (RBF / Gaussian Kernel)**\n",
        "\n",
        "* **Formula:**\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n",
        "$$\n",
        "\n",
        "* **Use Case:**\n",
        "\n",
        "  * Widely used when the **relationship between features and target is highly non-linear**.\n",
        "  * Example: Classifying points in a **circular pattern** in 2D space — not separable with a straight line.\n",
        "  * RBF maps the data to higher dimensions where a **linear hyperplane** can separate the classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Other Common Kernels**\n",
        "\n",
        "* **Linear kernel:** No transformation, used for linearly separable data.\n",
        "* **Polynomial kernel:** Maps data into polynomial feature space, useful for curved boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "* Imagine trying to separate data shaped like concentric circles.\n",
        "* In 2D, you cannot draw a straight line to separate them.\n",
        "* Kernel Trick **lifts the data into 3D** (or higher), where a plane can separate the classes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **What is a Naïve Bayes Classifier?**\n",
        "\n",
        "* A **Naïve Bayes (NB) classifier** is a **probabilistic machine learning algorithm** used for **classification tasks**.\n",
        "* It is based on **Bayes’ Theorem**, which calculates the **probability of a class given the features**:\n",
        "\n",
        "$$\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C$ = class label\n",
        "\n",
        "* $X$ = feature vector\n",
        "\n",
        "* $P(C|X)$ = posterior probability of class given features\n",
        "\n",
        "* $P(X|C)$ = likelihood of features given class\n",
        "\n",
        "* $P(C)$ = prior probability of class\n",
        "\n",
        "* $P(X)$ = evidence (normalizing factor)\n",
        "\n",
        "* The classifier **predicts the class with the highest posterior probability**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is it called “Naïve”?**\n",
        "\n",
        "* It assumes that **all features are independent given the class**.\n",
        "* In reality, features often have correlations, so this assumption is **“naïve”**.\n",
        "* Despite this strong assumption, Naïve Bayes often performs **very well in practice**, especially for text classification, spam detection, and medical diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Advantages**\n",
        "\n",
        "* Simple and fast to train.\n",
        "* Works well with **high-dimensional data**.\n",
        "* Performs surprisingly well even if the independence assumption is violated.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Use Case**\n",
        "\n",
        "* **Spam email detection:** Each word is treated as an independent feature to predict if an email is spam or not.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Gaussian Naïve Bayes**\n",
        "\n",
        "* **Assumption:** Features are **continuous** and follow a **Gaussian (normal) distribution**.\n",
        "* **Formula for likelihood:**\n",
        "\n",
        "$$\n",
        "P(x_i|C) = \\frac{1}{\\sqrt{2 \\pi \\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
        "$$\n",
        "\n",
        "Where $\\mu_C$ and $\\sigma_C^2$ are the mean and variance of the feature for class $C$.\n",
        "\n",
        "* **Use Case:**\n",
        "\n",
        "  * Predicting class labels based on **continuous data** like height, weight, temperature, or medical measurements.\n",
        "* **Example:** Predicting if a patient has a disease based on blood test results.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Multinomial Naïve Bayes**\n",
        "\n",
        "* **Assumption:** Features are **discrete counts**, often representing **frequency of events**.\n",
        "\n",
        "* **Formula:** Uses probabilities based on **count of each feature per class**.\n",
        "\n",
        "* **Use Case:**\n",
        "\n",
        "  * Text classification or document categorization where features are **word counts or term frequencies**.\n",
        "\n",
        "* **Example:** Spam email detection using **word frequency vectors**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Bernoulli Naïve Bayes**\n",
        "\n",
        "* **Assumption:** Features are **binary (0 or 1)**, representing the **presence or absence** of a feature.\n",
        "* **Use Case:**\n",
        "\n",
        "  * Text classification with **binary occurrence features** (word present or not).\n",
        "* **Example:** Email spam detection using **binary bag-of-words** (1 if word exists, 0 otherwise).\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table**\n",
        "\n",
        "| Variant     | Feature Type    | Example Use Case                          |\n",
        "| ----------- | --------------- | ----------------------------------------- |\n",
        "| Gaussian    | Continuous      | Predicting disease from lab test values   |\n",
        "| Multinomial | Discrete counts | Text classification with word counts      |\n",
        "| Bernoulli   | Binary features | Spam detection with word presence/absence |\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Intuition:**\n",
        "\n",
        "* **Gaussian:** Continuous data → use distribution of values.\n",
        "* **Multinomial:** Frequency counts → how many times each event occurs.\n",
        "* **Bernoulli:** Presence/absence → whether an event occurs or not.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OurR4bfYTFrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info: ● You can use any suitable datasets like Iris, Breast Cancer, or Wine from sklearn.datasets or a CSV file you have. Question 6: Write a Python program to: ● Load the Iris dataset ● Train an SVM Classifier with a linear kernel● Print the model's accuracy and support vectors.(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n",
        "print(\"\\nNumber of Support Vectors for each class:\", svm_model.n_support_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7UGmYWCURDF",
        "outputId": "fd8b14b1-d9ae-4fdf-c5c8-17a26a1ff945"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Number of Support Vectors for each class: [ 3 11 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to: ● Load the Breast Cancer dataset ● Train a Gaussian Naïve Bayes model ● Print its classification report including precision, recall, and F1-score.(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV-WxabRUbdu",
        "outputId": "871e9ed3-c8d7-45b6-be21-6711f925191e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to: ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ● Print the best hyperparameters and accuracy.(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = pd.Series(wine.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define SVM model\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on test set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOkm5KXfUii8",
        "outputId": "137569d2-8fca-4c48-deb8-025613e51a93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.01}\n",
            "Accuracy on test set: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ● Print the model's ROC-AUC score for its predictions.(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load a subset of 20 Newsgroups dataset (for simplicity, choose 3 categories)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.med']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers','footers','quotes'))\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = nb.predict_proba(X_test)\n",
        "\n",
        "# Binarize labels for multi-class ROC-AUC\n",
        "y_test_bin = label_binarize(y_test, classes=[0,1,2])\n",
        "\n",
        "\n",
        "# Compute ROC-AUC (macro-average for multi-class)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred_prob, average='macro', multi_class='ovr')\n",
        "print(\"ROC-AUC score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_K0-cADUsC4",
        "outputId": "836c143b-57d7-47ff-d099-4247563ebe69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 0.983642101133334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ● Text with diverse vocabulary ● Potential class imbalance (far more legitimate emails than spam) ● Some incomplete or missing data Explain the approach you would take to: ● Preprocess the data (e.g. text vectorization, handling missing data) ● Choose and justify an appropriate model (SVM vs. Naïve Bayes) ● Address class imbalance ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Preprocess the Data**\n",
        "\n",
        "1. **Handle missing values:**\n",
        "\n",
        "   * Replace empty emails with an empty string or remove them.\n",
        "2. **Text vectorization:**\n",
        "\n",
        "   * Convert raw emails into numerical features using **TF-IDF** or **CountVectorizer**.\n",
        "3. **Optional cleaning:**\n",
        "\n",
        "   * Lowercase, remove punctuation, stopwords, and perform tokenization.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Choose Model**\n",
        "\n",
        "* **Naïve Bayes (MultinomialNB)** is often preferred for text classification because:\n",
        "\n",
        "  * Assumes independent word occurrences (works well with high-dimensional text).\n",
        "  * Fast to train and performs well even with small datasets.\n",
        "* **SVM** can also work, but it’s slower on very large datasets and requires more tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Address Class Imbalance**\n",
        "\n",
        "* Techniques:\n",
        "\n",
        "  * **Class weighting:** `class_weight='balanced'` in SVM or resampling techniques.\n",
        "  * **Oversampling minority class:** e.g., SMOTE or simple duplication.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Train Model & Evaluate**\n",
        "\n",
        "* Metrics:\n",
        "\n",
        "  * **Precision:** Minimize false positives (legitimate emails marked as spam).\n",
        "  * **Recall:** Minimize false negatives (spam emails not detected).\n",
        "  * **F1-score:** Balance precision and recall.\n",
        "  * **ROC-AUC:** Evaluate separability of classes.\n",
        "\n",
        "---\n",
        "\n",
        "## **Python Implementation Example**\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Example synthetic dataset\n",
        "data = pd.DataFrame({\n",
        "    'email': [\n",
        "        \"Win a free iPhone now\",\n",
        "        \"Meeting at 10 am\",\n",
        "        \"Get cheap meds online\",\n",
        "        \"Project deadline extended\",\n",
        "        \"Congratulations! You've won\",\n",
        "        \"Lunch with client tomorrow\"\n",
        "    ],\n",
        "    'label': [\"spam\", \"ham\", \"spam\", \"ham\", \"spam\", \"ham\"]\n",
        "})\n",
        "\n",
        "# Handle missing emails\n",
        "data['email'] = data['email'].fillna(\"\")\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data['email'])\n",
        "\n",
        "# Encode labels\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(data['label']).ravel()\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_prob = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=['ham','spam']))\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```\n",
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "         ham       1.00      1.00      1.00         1\n",
        "        spam       1.00      1.00      1.00         1\n",
        "\n",
        "ROC-AUC Score: 1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Business Impact**\n",
        "\n",
        "* **Automated spam detection:** Reduces user exposure to malicious or unwanted emails.\n",
        "* **Improves productivity:** Users focus on important emails.\n",
        "* **Cost-effective:** Reduces need for manual filtering.\n",
        "* **Customer trust:** Increases satisfaction by ensuring inbox quality.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "g-apzLwQZ_4n"
      }
    }
  ]
}